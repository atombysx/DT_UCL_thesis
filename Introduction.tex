\chapter{Introduction to Visual Navigation}
\label{chapterlabel1}

\section{Visual Navigation}
Before the scientific research in visual illusion, artists like Escher have already tricked the audience with oddly interesting competition between local and global geometric perception of the visual space. This example demonstrates how limited animal vision is and relying on the egocentric continuous updates from our retina cannot fully capture global spatial context of the environment. Imagine a person stands in the middle of a symmetrical square by north-south. The person sees an identically looking pigeon when the person looks both north and south. How does one tell that the identical images are different in the spatial context? If the person walks to a completely identical square next to this square, how does one tell the squares are different in the global space context? A 'cognitive map' or a 'spatial representation' of the global context in the brain is required to distinguish the allocentric difference from identical egocentric perspective. Therefore, the visual system and navigational system in the brain need to work together coherently for the animal to explore and navigate in space. Vision and navigation have been separate research directions for decades, since the early discoveries of visually selective primary visual cortical neurons and place cells in hippocampus. However, most research in vision in the past have been focusing on how visual neurons along the visual processing pathways respond passively to stationary or moving stimuli, though animals use vision and other sensory modalities in action, for instance, exploring and navigating in an environment to find food. Navigation is a critical behavioural process to traverse from one location to another and the hippocampus is thought to be a 'cognitive map' guiding the animal's action in space. Such a cognitive map integrate multi-sensory information and self-motion information into coherent representations of the surroundings. Out of these multi-sensory modalities, vision provides detailed spatial information in the external world for the animal to have s stable sense of the environment. There is a rich literature of functional tuning in visually responsive cells and spatially tuned cells in both visual areas and spatial areas. In rodent studies, manipulation of visual landmarks can shape and alter spatial tuning of cells in hippocampal areas. In addition, there is strong anatomical evidence that visual areas project to navigational areas. Visual cortical neurons in mice also show navigational signals during visually guided navigation tasks and anatomical evidence suggests that there are projections from navigational areas to visual cortical areas. However, little is known about relationship between visual and spatial representations in the brain. In this thesis, I will examine the dynamically changing visual and spatial representations during visual navigation thanks to recent advances in large-scale neural recordings and virtual reality (VR) system for mice. Hence, in this introduction, I will first discuss the basis of visual processing for navigation from a traditional visual neuroscience perspective. Secondly, I will examine the role and impact of visual information in navigation. Third, I will use the recent advances in cognitive and behavioral influences in the visual cortex of mouse to bring out the exciting new direction of connecting vision and navigation together using visual navigation.

\section{Visual Processing for Navigation}
\subsection{Visual Cortical Processing}
Vision is traditionally viewed as an important input to navigational areas to provide visual information in a feedforward pathway supported by functional and anatomical evidence. In mammalian visual system,  retinal images project to Lateral Geniculate Nucleus (LGN) in the thalamus and Superior Colliculus (SC) in the midbrain. The LGN further projects to the visual primary cortex (V1). The classical view of the visual cortex is that the V1 receives a relay of retinal image processed by LGN with the matched retinotopic map, and V1 encodes the visual space and further projects to more specialised higher visual areas (HVAs). The retinal images are still two-dimensional, but the visual space is more complex. Along the anatomical projections from retina to HVAs, there are parallel lines of processing regarding the visual space which has two major aspects: first, the spatial location of the visual field, that is, the receptive field and it becomes larger in cells along the hierarchy; second, the spatial and temporal components of the visual features within the visual field. Combining such diverse visual feature tunings in visual cortical neurons can achieve many perceptual functions, including contrast sensitivity, color perception, local and global motions, and object recognition. The functional tunings to these visual modalities are found in two anatomical streams of higher visual area hierarchies starting from V1, dorsal and ventral streams. The ventral streams encodes visual modality about 'what', including shapes, colors, objects, even faces. The dorsal stream encodes 'where', such as local motion, global motion, optic flow. Spatial feature functions such as object recognition can help animals recognise spatial landmarks in an environment, and visual motion and optic flow can help animals encode the amount of spatial changes when the animal explores in an environment. Anatomically, in primates, Felleman and Van Essen et al. maps out the connections between all visually relevant cortical areas. Such complicated inter-area connections are dense and the boundary between visual parallel pathways is vague. These connections eventually project to entorhinal cortices and hippocampus in the hierarchy and suggest that all of the pathways can provide unique visual feature information about the spatial environment. 

Mice have a similar and less hierarchical architecture where V1 projects to HVAs. Most studies consider the lateral HVAs as the ventral stream, the 'what' pathway, and medial HVAs as the dorsal stream, the 'where' pathway. Both pathways project to the retrosplenial(RSP) cortex and medial entorhinal cortex (MEC) which provide feedforward input to the hippocampus. The retrosplenial cortex is considered as an area which transforms egocentric framework to allocentric framework using visual objects, visual motion, locomotion, and heading direction, and the medial entorhinal cortex  integrates sensory information and self-motion information to estimate path positions. However, the retrosplenial cortex receives projections from V1 and HVAs whereas MEC has biased input from the lateral HVAs ('what' pathway) and input from RSP directly. In addition, \cite{dubanet_retrosplenial_2024} shows MEC responds to drifting gratings and suppressing RSP can suppress it. Therefore, the 'what' pathway directly projects to MEC and 'where' pathway routes to RSP which further projects to MEC. This anatomical architecture implicates how different visual features feed into navigational areas. 

However, little is known about how functional information is communicated between these divisions along the hierarchy. Recently, \cite{semedo_cortical_2019} looked at interactions between neural populations in primate's V1, V2 and V4. They reveal the feedforward and feedback communications through distinct subspaces carrying low-dimensional and selective visual information. This novel analysis method provides insights on how selective information can be provided to areas responsible for higher-order cognition and the information propagated cannot be taken for granted through anatomical connection evidence. Overall, above review shows that cortical visual processing has high-level of specialization in visual features needed for navigation and indicates that understanding visual processing for navigation requires detailed and well-controlled experimental designs to explore how the brain uses visual information during navigation.



\subsection{Using Mouse Model for Studying Visual Processing During Navigation}
However, vision science is heavily focused on primate and cat electrophysiology and human psychophysics which require the observers to be stationary with well-controlled visual stimuli. This is due to the difficulty of tracking what the observer is seeing and most of vision studies are open-loop experiments that the animals passively receive visual information. In contrast, there are huge leaps in using mouse as a model for vision thanks to advances in both large-scale recording systems looking at hundreds and thousands of neurons at the same time and diverse genetic tools, and virtual reality (VR) as a tool to explore mouse's behaviour. A VR commonly has two components: first is that the animal is headed fixed on an air ball or a running wheel to freely run; second, a closed-loop system in sync with the animal's running speed. The closed-loop system allows the animal to interact with the virtual environment finely controlled by the experimenters. VR is a great tool to have a reliable set of trackable variables and has been used in navigation studies in mice to understand how neural dynamics can change by manipulations of the spatial environments including changing environments, conflicts between self motion and the environment, changing only parts of the environmental features. Therefore, VR opens up a way to study visual processing for navigation. With VR, the animal is head-fixed with trackable records of what the animal is seeing in real-time, and locomotion and eye tracking are available. With the aid of VR, one can find out about what visual features are being used by the mouse to navigate in the virtual environment by changing gain, switching environments, moving landmarks to dissociate variables. To better design experiments for these manipulations in VR, understanding how visual information can impact spatial representation is important.


\section{Visual Information in Navigation}
\subsection{Spatial Representations Rely on Sensory Modalities}
For an animal to travel between two points in space, it requires a reliable representation of the space it is in to accurately arrive at the right position. Hippocampus is the first region found to have place cells stably tuned to a spatial location in the environment and more work has shown hippocampus is important for spatial, contextual, episodic, emotional memory as well - forming the theory of hippocampus as a 'cognitive map'. In addition, neural correlates of  reward, evidence, time are found in the hippocampus and they are commonly associated variables in naturalistic behaviour of animals. These place cells together form a stable spatial representation of the environment by discharging at different place fields. These place fields tile up the entire environment. In the parahippocampal areas, there are also cells tuned to boundary vector, object vector, head direction, and even hexagonal grids tiling in the environment which are the grid cells in the medial entorhinal cortex (MEC). They are connected to the hippocampus as input and output and MEC is one of the primary cortical input to hippocampus. Grid cells have same spacing and orientation within the same grid modules along the dorsal-ventral axis and cover up the phases the periodic firing patterns. Grid cell and place cell populations interact bidirectionally. The cognitive map of the environment, that is spatial representation, is not independent of experience and requires continuous updates from sensory modalities, including self-motion information and allothetic information.

Path integration is a classic behaviour studied to understand spatial representations which integrates self-motion information to update the animal's position in the brain without any external cues \cite{mcnaughton_path_2006, etienne_path_2004}. When an animal moves, multiple sensory modalities, including vision, motor and vestibular system, feedback to reflect how much the animal has moved. Such feedback allows computations of higher-order features of self-motion and place cells and grid cells can integrate them to update the animal's position. By lesions of these structures, rodents perform path integration with higher errors. Grid cells in MEC are suggested to support path integration by integrating self-motion and external cues can reduce the errors.

External cues are important to form a stable spatial representation, since path integration solely with self-motion information can have errors. Geometry of an environment can change the place field size and shape of the place cells \cite{okeefe_geometric_1996} and grid cells rescale under similar manipulations \cite{barry_experience-dependent_2007}. More than just visual spatial information, both olfactory and tactile cues can be used to stablise or change place fields \cite{gener_tactile_2013, zhang_spatial_2015}. Though both allothetic and self-motion information are shown to impact spatial representations, little is known how they are integrated together to have a unified representation of all these variables. Hippocampus does not directly receive the external sensory information but MEC is suggested to be an important hub for integrating both self-motion and external sensory information together.

\subsection{MEC Integrates Self-Motion and Visual Information}
MEC grid cells fire periodically in hexagonal grid patterns and are found to perform path integration which is a behavioural function to integrate self-motion information only to track the animal's location. Grid cells are commonly modeled with a 2D neural sheet of continuous attractor which can solve path integration with the grid patterns by revolving along the neural sheet. However, grid cells are not the only functional cell type in the MEC. There are many other types of cells to navigational variables, including speed, head direction, border, and even object. Across these functional types, many cells are tuned to multiple variables at the same time \cite{hardcastle_multiplexed_2017, sargolini_conjunctive_2006}. Both light in the room and high running speed can enhace the grid patterns in grid cells \cite{chen_absence_2016, hardcastle_multiplexed_2017}. In addition, cue-cells tuned to visual or auditory cues are also reported. The tuning to self-motion related variables, such as speed, head direction, grid cells, can be an integral of three types of self-motion cues which are the locomotion of the animal, optic flow from visual motion, and vestibular movements. On top of the self-motion, removing visual input can disrupt grid cell firing in mice \cite{chen_absence_2016} and even place cells can be distorted by manipulations of visual cues in 2D VR \cite{chen_how_2013}. In addition, boundary and object tunings are, in nature, closely related to external sensory cues as well. 

In recent 10 years, a new direction of dissecting external cues and self-motion coding in MEC emerges after the advances in VR. The advantage of VR is that the experiments can have fine control of sensory cues, gain of the wheel, and reward locations, and precise records of these variables and the animal's behavioural variables. By using combinations of visual and sensory cues at various locations, MEC cells can respond differently to both types of cues, and some cells only respond to either visual or auditory information whereas some others respond to both \cite{nguyen_medial_2024}. By varying the gain of the VR, border cells are found to be stable whatever the gain is but both grid cells and speed cells can be influenced by both locomotion and visual information \cite{campbell_principles_2018}. \cite{wen_one-shot_2024} found grid cells in 1D VR fire peridically at synchronised freqeuncies corresponding to grid modules and introduction of visual floor and landmark cues can rapidly induce more stable firing patterns across trials. In addition, these stable firing patterns shift when the landmark positions are shifted. Both \cite{campbell_principles_2018} and \cite{wen_one-shot_2024} fit a 2D continuous attractor model to these data and found that visual landmarks can distort grid cell network. This indicates that grid cell network anchors to external cues and form spatial representation according to them.

MEC spatial representation requires multi-sensory information but also these information can induce contextual changes in spatial representations.

\subsection{Spatial Representations of Different Spaces}
When an animal travels between environments, over experience, the animal can contextually distinguish the environments using external cues. A phenomenon called 'remapping' occurs when the animal moves to a new environment that the same place cell has different firing patterns from the previous environment. The MEC grid cells also coherently remap. Hence, the brain forms separate spatial representations of the environments and can be considered as pattern separation in hippocampal function\cite{rolls_mechanisms_2013}. In addition to the pattern separation, over experience, the spatial representations can be recalled by pattern completion with particular environmental cues.

Sensory manipulations are commonly used to induce a global contextual difference and cause remapping in place cells and grid cells. Changing the geometry of the surrounding environment between square and circular shapes can directly cause place cells to remap \cite{wills_attractor_2005} and changing wall color and odor can induce remapping in hippocampus as well \cite{anderson_heterogeneous_2003, zhang_spatial_2015}. In addition, hippocampal place cells have rate remap that preserves the place field location but change in firing rate in a familiar environment with minor changes \cite{fyhn_hippocampal_2007}. In MEC, whenplace cells remap globally, the grid cells have realignment with shifts in position and orientation while reserving the internal structure, but when place cells have rate remapping, grid cells stay the same. \cite{fyhn_hippocampal_2007}. In deformation of environment boundaries without changing shape of boundaries, grid patterns rescale according to the change in boundaries parametrically and persist for days \cite{barry_experience-dependent_2007}. In a VR study, mouse grid cells rapidly remap when novel landmarks are introduced along the VR with multiple stable firing fields and changing the same landmark set's location can shift the firing fields in similar distance \cite{wen_one-shot_2024}. In addition, slow running speed and can induce remapping in MEC cells \cite{low_dynamic_2021}. These fast dynamics of changing representations show the flexibility for the brain to act in a dynamic environment.

\section{When Vision Knows Where}
\subsection{Distributed Codes of Task Variables in Visual Cortex}
With advancements in large scale recordings in both electrophysiology and optical techniques, populations of neurons across the visual cortex and beyond were found to show neural correlates of task variables and cognitive influences.

The best advantage of VR is that the mouse can choose to stay still or run. \cite{saleem_integration_2013} showed mice V1 neurons are tuned to running speed in darkness and the visual responses to landmarks in a VR corridor are modulated by running speed. Further to that, V1 neurons respond differently at single neuron and population level to the visual dots moving at the same speed between running and stationary \cite{horrocks_flexible_2024}. In addition, mismatch between locomotion and visual stimuli by halting the visual stimuli in a VR when the mouse runs can produce perturbation responses in V1 and can be modulated by running as well \cite{muzzu_feature_2021, zmarz_mismatch_2016, keller_sensorimotor_2012}. Further to locomotion, animal's arousal state can increase gain and modulates surround suprression in V1 neuronal responses \cite{ayaz_locomotion_2013, polack_cellular_2013}, and arousal can modulate retinal output as well \cite{schroder_arousal_2020}. In addition to all these factors, \cite{stringer_spontaneous_2019} recorded from thousands of neurons in mouse visual cortex and other cortices and found \~60\% neurons correlates with arousal and other behavioural variables.

In active behavioural tasks, there is a new trend of finding distributed codes of task variables in different contexts. In visual detection tasks, a mosue is required to distinguish visual patterns and turn a wheel to choose them to obtain rewards. Multiple studies \cite{orlandi_distributed_2023, steinmetz_distributed_2019,zatka-haas_sensory_2021} found distributed signals of task variables, including wheel movements, saccades, stimulus identity, rewards, present across visual cortex and other cortices. \cite{orlandi_distributed_2023} particularly found a low-dimension representation of decision variables in the medial (ventral stream) visual areas. In VR active behaviour, \cite{tseng_shared_2022} found navigational task variables encoding across the visual cortex and posterior cortex as well, and there are regional specilisations in contributions to these variables. Further to task variables, contextual difference in an active behaviour can lead to different degrees of visual responses to the same stimuli and learning can reshape the tunings and manifolds of visual representations \cite{hajnal_continuous_2023, faulkner_context_2025, goltstein_mouse_2021}.

These modulatory influences are all related to navigation behaviour which requires contextual discrimination of global environments and local cues, and learns relationship between spatial information and rewards.

\subsection{Spatial Modulation}
Most VR navigation studies use sets of unique cues to maximise learning of the environment and look at the hippocampal-entorhinal circuit. To inspect the impact of spatial representation on sensory cortices, like visual cortex, visual landmark manipulation is required to compare how responses are modulated. \cite{saleem_coherent_2018} uses a novel design with only vertical grating landmarks (call it as A) and plaid pattern landmarks (call it as B). But these landmarks are placed in the arrangement of A-B-A-B at the positions 20, 40, 60, 80 cm which comprises two identical zones in the first and second halves of the VR. The VR also contains a reward zone that the animal needs to learn to lick actively to obtain an active reward. Therefore, the mouse needs to distinguish the first and second halves of the VR to only lick at the end of second half. Many neurons were found to respond differently to the same visual landmarks at different positions and position decoding using V1 neurons can distinguish the two identical regions in the VR. In addition, V1 and CA1 decoded positions and their errors are correlated. 

\cite{mika_diamanti_spatial_2021} investigated if such spatial modulation comes from early visual processing or it is purely cortical by imaging thousands of neurons in LGN, V1, and other HVAs. They use the exactly same VR design and let mice run in the VR over days without a task. A spatial modualtion index (SMI) is used to measure the degree of difference between visual responses to the same landmarks at different positions. It finds the maximum response in the VR from odd laps of the VR as the preferred resposne and calculates the difference between preferred responses and not preferred responses over the sum of the two from the even laps. This index tells how much a neuron favors one position of the same landmark over the other one. They compare the cumulative probability of the SMI between LGN, V1, and HVAs, and found LGN has no spatial modulation compared to visual cortical areas. Over days, V1 has higher degree of spatial modulation as well - the SMI cumulative probability distribution shifts to the positive side over days.

In addition to the funcitonal recordings, anatomically, thoug hippocampus doesn't have much direct connections to visual cortex, navigational areas like RSC, mEC and lEC have \~ 2-10\% projections to layers 2-3 and layers 5-6 in V1 but not layer 4 \cite{yao_whole-brain_2023, morimoto_organization_2021}.

Overall, there are spatial context influences to visual cortex as early as V1. Hence, the cortical  visual processing for navigation does not simply go in a bottom-up fashion and the visual processing and spatial cognition interact with each other in the brain. 

\section{Putting Vision and Navigation Together}
In above sections, I've reviewed vision and navigation, and they can influence each other. However, little is known about how these two systems interact with each other. In visual illusion, local visual feature conflicts can induce illusions in both visual motion and object perception. Global perceptions are a key to solve the competitions. \cite{saleem_coherent_2018, mika_diamanti_spatial_2021} similarly produced a local and global competition that the same visual landmarks can be placed at different locations. This requires the hippocampus, as the cognitive map, is aware of the difference in space. The spatial, contextual and task-related variables influences in the visual cortex indicate distributed codes might shape the representations of local visual features more similar to the global representation and this effect increases over experience. Therefore, sensory input dominates and shapes the cognitive map over learning and the global context in the cognitive map, in turn, influences the sensory input's representation manifolds to be consistent across brain regions. With this hypothesised theory, animal can use the global context to distinguish allocentric difference of an object and also self-motion induced visual motion and other objects' motion.

With the aid of neurpixels, one can record hundreds of neurons from brain regions from both visual areas and navigational areas and test how neural representations in these brain regions act and interact when the animal explore in dynamically changing environments.

\section{Visual Navigation in Dynamically Changing Environments}
In a VR setting, a mouse can explore in one environment and immediately teleport to a new environment. The cognitive map cannot immediately be aware of the change and requires some cues to pattern complete the other environment even if experienced. 



