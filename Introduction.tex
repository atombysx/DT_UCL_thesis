\chapter{1. Introduction to Visual Navigation}
\label{chapterlabel1}

\section{1.1 Visual Navigation}
Vision and navigation have been separate research directions for decades, since the early discoveries of visually selective primary visual cortical neurons and place cells in hippocampus. Navigation is a critical behavioural process to traverse from one location to another. Animals form neural circuits integrate multi-sensory information and self-motion information into coherent representations of the surroundings. Vision is a key sensory processing in animals that provides detailed spatial information in the external world for high-level perceptual demands, including navigation. There is a rich literature of functional tuning in visually responsive cells and spatially-tuned cells in both visual areas and spatial areas. In rodent studies, manipulation of visual landmarks can shape and alter spatial tuning of cells in hippocampal areas. In addition, there is strong anatomical evidence that visual areas project to navigational areas. Visual cortical neurons in mice also show navigational signals during visually-guided navigation tasks and anatomical evidence suggests there is projections from navigational areas to visual cortical areas. However, little is known about relationship between visual and spatial representations in the brain. In this thesis, I will examine the dynamically-changing visual and spatial representations during visual navigation thanks to recent advances in large-scale neural recordings.

\section{1.2 Visual Processing for Navigation}
In mammalian visual system,  retinal images project to Lateral Geniculate Nucleus (LGN) in the thalamus and Superior Colliculus (SC) in the midbrain. The LGN further projects to the visual primary cortex (V1). The classical view of the visual cortex is that the V1 receives a relay of retinal image processed by LGN with the matched retinotopic map, and V1 encodes the visual space and further projects to more specialised higher visual areas (HVAs). The retinal images are still two dimensional but the visual space is more complex. Along the anatomical projections from retina to HVAs, there are parallel lines of processing regarding the visual space which has two major aspects: first, the spatial location of the visual field, that is, the receptive field and it becomes larger in cells along the hierarchy; second, the spatial and temporal components of the visual features within the visual field. Combining such diverse visual feature tunings in visual cortical neurons can achieve many perceptual functions, including contrast sensitivity, color perception, local and global motions, and object recognition. The functional tunings to these visual modalities are found in two streams of higher visual area hierarchies starting from V1, dorsal and ventral streams. The ventral streams encodes visual modality about 'what', including shapes, colors, objects, even faces. The dorsal stream encodes 'where', such as local motion, global motion, optic flow. Such a separation has been proposed in the new popular animal model, mouse, as well. In mouse brain, the visual cortex is much shallower and the specilisations in HVA are still in debate to the extent that some argue the multiple HVAs identified before can be considered as V2 as a whole. Even though, the visual processing can be divided into different 



All kinds of visual areas have different kinds of representations for different purposes: give examples.

Then in the infmaous primate visual cortex anatomy figure, it ends at hippocampus which is important in navigation and memory. Many research has looked at memory tasks in higher visual areas and found xxxx.
However, it's difficult to study how visual representations are formed during navigation and how they can be useful for navigation since most visual neuroscience research uses head fixation to ensure the visual stimuli are well controlled and it's difficult to achieve during freely moving navigation. Now, with the advance in virtual reality (VR), there's more literature in studying how manipulations of sensory information in VR environments can impact animal's behaviour and spatial representations. VR also made it possible to present well-controlled visual stimuli as cues for animals to navigate while head-fixed. Overall, how can we make advantage of this to study visual navigation?

\section{1.3 Visual Information in Navigation}


\section{1.4 When Vision Knows Where}


\section{1.5 Putting Vision and Navigation Together}

Feedback projections cast distributed signals about space, reward, actions etc to modify representation manifolds of local sensory features and such modifications help separate the local representations of the same feature under different cognitive demands. Such modifications by distributed codes coordinate between representations across sensory areas to create a consistent global perceptual representation which improves the accuracy of encoding perceptual features for cognitive tasks. Predictive coding assumes a feedback hierarchy which can be too slow and can be chaotic when too many conflicts.
\section{1.6 Visual Navigation in Dynamically Changing Environments}

Some stuff about things.\cite{example-citation} Some more things. 

Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.
\blindtext
