\chapter{Introduction to Visual Navigation}
\label{chapterlabel1}

\section{Visual Navigation}
Vision and navigation have been separate research directions for decades, since the early discoveries of visually selective primary visual cortical neurons and place cells in hippocampus. Navigation is a critical behavioural process to traverse from one location to another. Animals form neural circuits integrate multi-sensory information and self-motion information into coherent representations of the surroundings. Vision is a key sensory processing in animals that provides detailed spatial information in the external world for high-level perceptual demands, including navigation. There is a rich literature of functional tuning in visually responsive cells and spatially-tuned cells in both visual areas and spatial areas. In rodent studies, manipulation of visual landmarks can shape and alter spatial tuning of cells in hippocampal areas. In addition, there is strong anatomical evidence that visual areas project to navigational areas. Visual cortical neurons in mice also show navigational signals during visually-guided navigation tasks and anatomical evidence suggests there is projections from navigational areas to visual cortical areas. However, little is known about relationship between visual and spatial representations in the brain. In this thesis, I will examine the dynamically-changing visual and spatial representations during visual navigation thanks to recent advances in large-scale neural recordings. Hence, in this introduction, I will first discuss about the basis of visual processing for navigation from traditional visual neuroscience perspective. Secondly, I will discuss about the role of visual information in navigation. Third, I will discuss about recent advances in cognitive and behavioural influences in visual cortex of mouse and the exciting new direction of connecting vision and navigation together using visual navigation.

\section{Visual Processing for Navigation}
Vision is traditionally viewed as an important input to navigational areas to provide visual information in a feedforward pathway supported by functional and anatomical evidence. In mammalian visual system,  retinal images project to Lateral Geniculate Nucleus (LGN) in the thalamus and Superior Colliculus (SC) in the midbrain. The LGN further projects to the visual primary cortex (V1). The classical view of the visual cortex is that the V1 receives a relay of retinal image processed by LGN with the matched retinotopic map, and V1 encodes the visual space and further projects to more specialised higher visual areas (HVAs). The retinal images are still two dimensional but the visual space is more complex. Along the anatomical projections from retina to HVAs, there are parallel lines of processing regarding the visual space which has two major aspects: first, the spatial location of the visual field, that is, the receptive field and it becomes larger in cells along the hierarchy; second, the spatial and temporal components of the visual features within the visual field. Combining such diverse visual feature tunings in visual cortical neurons can achieve many perceptual functions, including contrast sensitivity, color perception, local and global motions, and object recognition. The functional tunings to these visual modalities are found in two streams of higher visual area hierarchies starting from V1, dorsal and ventral streams. The ventral streams encodes visual modality about 'what', including shapes, colors, objects, even faces. The dorsal stream encodes 'where', such as local motion, global motion, optic flow. Spatial feature functions such as object recognition can help animals to recognise spatial landmarks in an environment, and visual motion and optic flow can help animals to decode the amount of spatial changes when the animal explores in an environment. Anatomically, in primates, et al maps out the connections between all visually relevant cortical areas. Such complicated inter-area connections are dense and the boundary between visual parallel pathways is vague. These connections eventually project to entorhinal cortices and hippocampus in the hierarchy and suggest that all of the pathways can provide unique visual feature information about the spatial environment. However, little is known about how functional information is communicated between these divisions along the hierarchy. Recently, there are attempts to look at interactions between neural populations in primate's V1, V2 and V4. They reveal the feedforward and feedback communications through distinct subspaces carrying low-dimensional and selective visual information. This novel analysis method provides insights on how selective information can be provided to areas responsible for higher-order cognition and the information propagated cannot be taken for granted through anatomical connection evidence. Overall, above review shows that cortical visual processing has high-level of specialization in visual features needed for navigation and indicates that understanding visual processing for navigation requires detailed and well-controlled experimental designs to explore how the brain uses visual information during navigation.

However, vision science is heavily focused on primate and cat electrophysiology and human psychophysics which require the observers to be stationary with well-controlled visual stimuli. This is due to the difficulty of tracking what the observer is seeing and most of vision studies are open-loop experiments that the animals passively receive visual information. In contrast, there are huge leaps in using mouse as a model for vision thanks to advances in both large-scale recording systems looking at hundreds and  thousands of neurons at the same time and diverse genetic tools, and virtual reality (VR) as a tool to explore mouse's behaviour. A VR commonly has two components: first is that the animal is headed to a ball or a running wheel to freely run; second, a closed-loop system in sync with the animal's running speed. The closed-loop system allows the animal to interact with the virtual environment controlled by the experimenters which can be a space with all kinds of visual cues. VR is a great tool to have a reliable set of trackable variables and has been used in navigation studies in mice to understand how neural dynamics can change by manipulations of the spatial environments including changing environments, conflicts between self motion and the environment, changing only parts of the environmental features. Therefore, VR opens up a way to study visual processing for navigation. With VR, the animal is head-fixed with trackable records of what the animal is seeing in real-time, and locomotion and eye tracking are available. In addition, even though mice have low acuity vision and the higher visual areas are less well-defined, mouse brain has a hierarchical visual processing and many recent studies indicate it follows a similar fashion of parallel processing in terms of what and where. Anatomically, mouse cortical visual processing also routes from V1 to multiple HVAs which further project to navigational areas including retrosplenial cortex, entorhinal cortices.

With the aid of VR, one can find out about what visual features are being used by the mouse to navigate in the virtual environment. In past navigation studies in rodents, real-time tracking of what the animal is seeing in an environment is not well-controlled but there are hints of what visual information is important for navigation. 


\section{Visual Information in Navigation}

Visual information is a key component for an animal to form a stable spatial representation of an environment. Hippocampus is the first region found to have place cells stably tuned to a spatial location in the environment and more work has shown hippocampus is important for spatial, contextual, episodic, emotional memory as well. In the parahippocampal areas, there are also cells tuned to boundary vector, object vector, head direction, and even hexagonal grids tiling in the environment which are the grid cells in the medial entorhinal cortex (MEC) while the animal navigaates between . Over decades, vast literature uses environmental feature manipulations to understand the various types of spatially-tuned cells. 

\section{When Vision Knows Where}
Not only that navigation requires visual information, navigation can influence visual processing as well. With the help of large-scale recording systems, experimenters can record hundreds to thousands of neurons simultaneously in multiple brain regions. With the rich datasets, many new studies indicate that neurons across cortical areas can encode variables in a behavioural task. More specifically to navigation, 

\section{Putting Vision and Navigation Together}

Feedback projections cast distributed signals about space, reward, actions etc to modify representation manifolds of local sensory features and such modifications help separate the local representations of the same feature under different cognitive demands. Such modifications by distributed codes coordinate between representations across sensory areas to create a consistent global perceptual representation which improves the accuracy of encoding perceptual features for cognitive tasks. Predictive coding assumes a feedback hierarchy which can be too slow and can be chaotic when too many conflicts.
\section{Visual Navigation in Dynamically Changing Environments}

Some stuff about things.\cite{example-citation} Some more things. 

Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.
\blindtext
